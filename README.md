
# ğŸ” RAG-Based Search Engine

This project is a **RAG (Retrieval-Augmented Generation)**-powered search engine designed to provide precise answers and references from regulatory documents related to nuclear safety. The system crawls safety datasets, processes them, and builds a search interface where users can input questions and receive relevant responses with cited references.

---

## ğŸ“¦ Project Structure

### 0. **Requirements**
Ensure that the required packages and dependencies are installed by referencing the `requirements.txt` file.

### 1. **Dataset Collection** ğŸ—‚ï¸

We collect nuclear safety regulation datasets from the NRC website.

- **`get_dataset.py`**:  
   This script crawls the NRC website for regulatory documents and stores them in the `./database/manual` directory. Any failed downloads are logged in `failed_downloads.txt`. Additionally, it saves the file path and corresponding file title in `file_title.csv`.
  
- **`check_dataset.py`**:  
   This script checks the `./database` folder for any corrupted or missing files and ensures that the dataset is intact.

### 2. **Run Search Engine** ğŸ”

- **`app.py`**:  
   This script runs the web application. When launched, it starts a local web server where users can input queries. By pressing the Enter key, the system provides an answer along with references to relevant documents.

### 3. **Functions** âš™ï¸

- **`functions/functions_rag.py`**:  
   This file stores all functions related to **RAG (Retrieval-Augmented Generation)** processing, including document chunking, encoding, and retrieval.

- **`functions/functions_utils.py`**:  
   This file stores utility functions such as file loading, dataset handling, and metadata processing.

### 4. **Frontend Design** ğŸ¨

- **`static/`** and **`templates/`**:  
   These folders contain the HTML, CSS, and other static assets for the web application design. The user interface is clean, minimalistic, and responsive.

---

## ğŸ› ï¸ Setup Guide

### 1. Clone the Repository
```bash
git clone https://github.com/separk-1/Inferring-Cause-of-Reactor-Overheat-Issues-Using-NLP.git
cd Inferring-Cause-of-Reactor-Overheat-Issues-Using-NLP
```
(I will change my repository name to RAG-Based Search Engine after the part 2 report is graded.)

### 2. Install the Required Packages
Make sure you have Python 3.10 and the required libraries installed. Install dependencies using:

```bash
conda env create -f requirements.txt
```

### 3. Download Spacy Language Model
To ensure proper NLP operations, download the Spacy language model:

```bash
python -m spacy download en_core_web_sm
```

### 4. Run Dataset Collection
To crawl and store the dataset from the NRC website:

```bash
python get_dataset.py
```

### 5. Check the Dataset
After downloading, run the script to verify the integrity of the dataset:

```bash
python check_dataset.py
```

### 6. Configure Your OpenAI API Key
Make sure to store your OpenAI API key in the `.env` file.

Create or update the `.env` file in the root directory with the following content:

```bash
OPENAI_API_KEY='your_openai_api_key'
```

### 7. Run the Search Engine
Launch the search engine locally:

```bash
python app.py
```

Open a browser and navigate to `http://127.0.0.1:5001` to access the web interface, ask questions, and view the generated answers with references.

---

## ğŸ—‚ï¸ Example File Structure
```
RAG-Search-Engine/
â”‚
â”œâ”€â”€ database/
â”‚   â””â”€â”€ manual/
â”‚       â”œâ”€â”€ NUREG0800_Chapter5/  # Crawled dataset
â”‚       â””â”€â”€ ...
â”œâ”€â”€ static/                      # Frontend assets (CSS, etc.)
â”œâ”€â”€ templates/                   # HTML files for the web app
â”œâ”€â”€ functions/
â”‚   â”œâ”€â”€ functions_rag.py         # RAG-related functions
â”‚   â””â”€â”€ functions_utils.py       # Utility functions
â”œâ”€â”€ get_dataset.py               # Script to crawl and download the dataset
â”œâ”€â”€ check_dataset.py             # Script to verify the dataset integrity
â”œâ”€â”€ app.py                       # Main web application script
â””â”€â”€ requirements.txt             # List of required dependencies
```

---

## ğŸ¯ Key Components

### ğŸ”‘ Dataset Collection & Preprocessing
- **Web Crawling**: The system scrapes regulatory documents from the NRC website and stores them in the local database.
- **Data Integrity Check**: It ensures the downloaded documents are intact.

### ğŸ”‘ RAG Search Engine
- **Document Chunking**: Documents are split into manageable chunks for efficient retrieval.
- **Question-Answering**: A combination of retrieval (via FAISS) and generative models provides answers to user queries, ensuring both relevance and accuracy.

---

## ğŸš€ How It Works

- **Document Encoding**: The documents are split into chunks, and embeddings are created using OpenAI embeddings, which are then stored in the FAISS vector store.
- **User Query**: The user enters a query through the web interface.
- **RAG Processing**: The system retrieves relevant documents based on the query and generates an answer.
- **References**: The response is accompanied by references to the original documents used to generate the answer.

---

## ğŸ“‹ Future Enhancements
- **Expand Dataset**: Integrate more comprehensive datasets beyond NRC regulations.
- **Refine Search Algorithm**: Improve retrieval and generation accuracy by experimenting with more advanced vector models.
- **User Feedback**: Implement feedback loops for users to rate the relevance of the results.

---

This project demonstrates how RAG can be used to build powerful question-answering systems based on unstructured text data. By leveraging NRC regulatory documents, this system provides accurate and detailed answers, making it a valuable tool for understanding nuclear safety regulations.
